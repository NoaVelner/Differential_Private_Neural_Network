{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T16:19:05.393142800Z",
     "start_time": "2024-07-25T16:19:05.347122900Z"
    }
   },
   "id": "b59d16b20e3ffcef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction To neural Networks  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed0d840c2d252c6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is a Neural Network?\n",
    "A neural network is a series of algorithms that attempt to recognize underlying relationships in a set of data through a process. The network consists of layers of nodes, or neurons, each performing simple computations and passing the results to the next layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc80eea718403aa5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Components of a Neural Network \n",
    "1. Neurons: Basic units (node) of a neural network that receive input, process it, and pass it on to other neurons. Each neuron has weights and biases that are adjusted during training to minimize errors. The neurons organized in layers. \n",
    "\n",
    "\n",
    "\n",
    "2. Layers: There is three types of Layers:\n",
    "    - Input Layer: The first layer that receives the input data.\n",
    "    - Hidden Layers: Intermediate layers that process inputs from the input layer. There can be one or more hidden layers in a neural network.\n",
    "    - Output Layer: The final layer that produces the output predictions.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "206620def2c9f0a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss\n",
    " The difference between the actual output and the predicted output is calculated using a loss function. This error measure guides the learning process.\n",
    " \n",
    " \n",
    "We can think at loss that simply sums the ${\\ell}_2$ distance between the predicted output form the actual output:\n",
    "$MSE={\\frac 1 n} \\sum ^n _{i=1} (y_i - \\hat y_i)^2$\n",
    "\n",
    "In neural network, we will use the \"Cross-Entropy Loss\".\n",
    "Cross-Entropy Loss: Used for classification tasks, it measures the difference between two probability distributions – the true labels and the predicted probabilities\n",
    "${\\text {Cross-Entropy Loss} = -\\sum ^n _{i=1}[y_i \\cdot log(\\hat {y_i})+(1-y_i)\\cdot log(1-\\hat{y_i})]$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f11109cd5a39fd2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Role in Training: The loss function guides the optimization process. During training, the goal is to minimize the loss, which means making the predictions as accurate as possible.\n",
    "\n",
    "- Loss Curve: A plot of loss versus training epochs can help visualize how well the model is learning. A decreasing loss indicates that the model is improving."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eb94c5c76a253d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task: implement cross entropy loss.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c29206c89ba10f7"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def crossEntropy_loss(output, targets, buffer):\n",
    "    \"\"\"\n",
    "    Calculates the categorical cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        output (Tensor): Model predictions of shape (num_samples, output_size).\n",
    "        targets (Tensor): Target labels of shape (num_samples, output_size).\n",
    "\n",
    "    Returns:\n",
    "        float: Categorical cross-entropy loss value.\n",
    "    \"\"\"\n",
    "    # buffer = 1e-10\n",
    "    loss = -np.mean(targets * np.log(output + buffer))\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T16:19:05.424420600Z",
     "start_time": "2024-07-25T16:19:05.362478600Z"
    }
   },
   "id": "aa9a03c8605c4e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Another way to measure our model, is $accuracy$.\n",
    "\n",
    "Accuracy is a metric used to evaluate the performance of a classification model. It measures the proportion of correct predictions out of the total number of predictions.\n",
    "\n",
    " Accuracy is calculated as the number of correct predictions divided by the total number of predictions. It is often expressed as a percentage.\n",
    " \n",
    "$Accuracy = {\\frac {\\text {Number of Correct Predictions}} {\\text {Total Number of Predictions}} \\cdot 100}$ \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f42d1a90e808bf55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If $y_i$ is the actual label and $\\hat {y_i}$ is the predicted label for $n$ samples:\n",
    "$Accuracy = {\\frac 1 n} \\sum ^n _{i=1} \\mathds{1}(y_i = \\hat{y_i})$ , where 1 is the indicator function, which is 1 if the condition is true and 0 otherwise."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ce71e550e21061"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Implement the following accuracy function for classification problem (possible labels are 0,1) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4e592798a102027"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def accuracy(output: np.ndarray, targets: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the model predictions.\n",
    "\n",
    "    Args:\n",
    "        output (Tensor): Model predictions of shape (num_samples, output_size).\n",
    "        targets (Tensor): Target labels of shape (num_samples, output_size).\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy value.\n",
    "    \"\"\"\n",
    "    predicted_labels = np.argmax(output, axis=1)\n",
    "    true_labels = np.argmax(targets, axis=1)\n",
    "    return np.mean(predicted_labels == true_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T16:19:05.424420600Z",
     "start_time": "2024-07-25T16:19:05.362478600Z"
    }
   },
   "id": "ecbde8ba1c82533d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation functions\n",
    "Activation functions play a crucial role in neural networks by introducing non-linearity into the model. This non-linearity allows the network to learn and model complex relationships between inputs and outputs. Here’s a detailed overview of various activation functions and their roles:\n",
    "\n",
    "1. Non-linearity: Without activation functions, a neural network would simply perform linear transformations, making it incapable of solving non-linear problems.\n",
    "2. Enabling Learning: Activation functions enable backpropagation by providing gradients needed for updating weights.\n",
    "3. Controlling Outputs: They help in squashing the output to a specific range, making the network's behavior more predictable and stable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71a8997e542557da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Common Activation Functions:\n",
    "- sigmoid/logistic function: $\\sigma (x) =  \\frac 1 {1+e^{-x}}$.\n",
    "  Properties:\n",
    "  - Outputs values between 0 and 1.\n",
    "  - Smooth gradient, preventing abrupt changes in output.\n",
    "- Rectified Linear Unit (ReLU):$\n",
    "\n",
    "\\text{ReLU}(x)\t=max\\left\\{ 0,x\\right\\} $.\n",
    "This function gives us:\n",
    "    - outputs values between 0 and infinity.\n",
    "    - Introduces sparsity by setting negative values to zero.\n",
    "- SoftMax\n",
    "    $\\text{Softmax}(x_i)=\\frac {e^{x_i}} {\\sum ^C _{j=1}e^{x_j}} \\text{(where C is the number of classes).}$\n",
    "    Properties:\n",
    "    - Outputs a probability distribution over classes.\n",
    "    - Commonly used in the output layer of multi-class classification problems\n",
    "    - Provides probabilistic interpretation.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a6203d465a8220c"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_values / np.sum(exp_values, axis=-1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T16:19:05.424420600Z",
     "start_time": "2024-07-25T16:19:05.383972900Z"
    }
   },
   "id": "8582d1083883d59e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient\n",
    "Gradient is a vector that represents the direction and rate of fastest increase of a function. In neural networks, it is used to adjust the weights and biases to minimize the loss function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc4df5f9646f7432"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T16:19:05.424420600Z",
     "start_time": "2024-07-25T16:19:05.393142800Z"
    }
   },
   "id": "e49b15edc4f6d1b8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
