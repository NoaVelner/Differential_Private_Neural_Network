{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T17:02:40.681658600Z",
     "start_time": "2024-07-25T17:02:40.616934400Z"
    }
   },
   "id": "b59d16b20e3ffcef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction To neural Networks  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed0d840c2d252c6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is a Neural Network?\n",
    "A neural network is a series of algorithms that attempt to recognize underlying relationships in a set of data through a process. The network consists of layers of nodes, or neurons, each performing simple computations and passing the results to the next layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc80eea718403aa5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Terms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "206620def2c9f0a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loss\n",
    " The difference between the actual output and the predicted output is calculated using a loss function. This error measure guides the learning process.\n",
    " \n",
    " \n",
    "We can think at loss that simply sums the ${\\ell}_2$ distance between the predicted output form the actual output:\n",
    "$MSE={\\frac 1 n} \\sum ^n _{i=1} (y_i - \\hat y_i)^2$\n",
    "\n",
    "In neural network, we will use the \"Cross-Entropy Loss\".\n",
    "Cross-Entropy Loss: Used for classification tasks, it measures the difference between two probability distributions – the true labels and the predicted probabilities\n",
    "${\\text {Cross-Entropy Loss} = -\\sum ^n _{i=1}[y_i \\cdot log(\\hat {y_i})+(1-y_i)\\cdot log(1-\\hat{y_i})]$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f11109cd5a39fd2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Role in Training: The loss function guides the optimization process. During training, the goal is to minimize the loss, which means making the predictions as accurate as possible.\n",
    "\n",
    "- Loss Curve: A plot of loss versus training epochs can help visualize how well the model is learning. A decreasing loss indicates that the model is improving."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eb94c5c76a253d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Task: implement cross entropy loss.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c29206c89ba10f7"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def crossEntropy_loss(output, targets, buffer):\n",
    "    \"\"\"\n",
    "    Calculates the categorical cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        Output (Tensor): Model predictions of shape (num_samples, output_size).\n",
    "        Targets (Tensor): Target labels of shape (num_samples, output_size).\n",
    "\n",
    "    Returns:\n",
    "        float: Categorical cross-entropy loss value.\n",
    "    \"\"\"\n",
    "    # buffer = 1e-10\n",
    "    loss = -np.mean(targets * np.log(output + buffer))\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T17:02:40.723734900Z",
     "start_time": "2024-07-25T17:02:40.625621900Z"
    }
   },
   "id": "aa9a03c8605c4e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accuracy \n",
    "Another way to measure our model, is $accuracy$.\n",
    "Accuracy is a metric used to evaluate the performance of a classification model. It measures the proportion of correct predictions out of the total number of predictions.\n",
    "Accuracy is calculated as the number of correct predictions divided by the total number of predictions. It is often expressed as a percentage.\n",
    " \n",
    "\n",
    "$Accuracy = {\\frac {\\text {Number of Correct Predictions}} {\\text {Total Number of Predictions}} \\cdot 100}$ \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f42d1a90e808bf55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If $y_i$ is the actual label and $\\hat {y_i}$ is the predicted label for $n$ samples:\n",
    "$Accuracy = {\\frac 1 n} \\sum ^n _{i=1} \\mathds{1}(y_i = \\hat{y_i})$ , where 1 is the indicator function, which is 1 if the condition is true and 0 otherwise."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ce71e550e21061"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Implement the following accuracy function for classification problem (possible labels are 0,1) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4e592798a102027"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "def accuracy(output: np.ndarray, targets: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the model predictions.\n",
    "\n",
    "    Args:\n",
    "        Output (Tensor): Model predictions of shape (num_samples, output_size).\n",
    "        Targets (Tensor): Target labels of shape (num_samples, output_size).\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy value.\n",
    "    \"\"\"\n",
    "    predicted_labels = np.argmax(output, axis=1)\n",
    "    true_labels = np.argmax(targets, axis=1)\n",
    "    return np.mean(predicted_labels == true_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T17:02:40.723734900Z",
     "start_time": "2024-07-25T17:02:40.653088300Z"
    }
   },
   "id": "ecbde8ba1c82533d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation functions\n",
    "Activation functions play a crucial role in neural networks by introducing non-linearity into the model. This non-linearity allows the network to learn and model complex relationships between inputs and outputs. Here’s a detailed overview of various activation functions and their roles:\n",
    "\n",
    "1. Non-linearity: Without activation functions, a neural network would perform linear transformations, making it incapable of solving non-linear problems.\n",
    "2. Enabling Learning: Activation functions enable backpropagation by providing gradients needed for updating weights.\n",
    "3. Controlling Outputs: They help in squashing the output to a specific range, making the network's behavior more predictable and stable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71a8997e542557da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Common Activation Functions:\n",
    "- sigmoid/logistic function: $\\sigma (x) =  \\frac 1 {1+e^{-x}}$.\n",
    "  Properties:\n",
    "  - Outputs values between 0 and 1.\n",
    "  - Smooth gradient, preventing abrupt changes in output.\n",
    "- Rectified Linear Unit (ReLU):$\n",
    "\n",
    "\\text{ReLU}(x)\t=max\\left\\{ 0,x\\right\\} $.\n",
    "This function gives us:\n",
    "    - outputs values between zero and infinity.\n",
    "    - Introduces sparsity by setting negative values to zero.\n",
    "- SoftMax\n",
    "    $\\text{Softmax}(x_i)=\\frac {e^{x_i}} {\\sum ^C _{j=1}e^{x_j}} \\text{(where C is the number of classes).}$\n",
    "    Properties:\n",
    "    - Outputs a probability distribution over classes.\n",
    "    - Commonly used in the output layer of multi-class classification problems\n",
    "    - Provides probabilistic interpretation.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a6203d465a8220c"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_values / np.sum(exp_values, axis=-1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T17:02:40.723734900Z",
     "start_time": "2024-07-25T17:02:40.653088300Z"
    }
   },
   "id": "8582d1083883d59e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Gradient\n",
    "Gradient is a vector that represents the direction and rate of the fastest increase a function. In neural networks, it is used to adjust the weights and biases to minimize the loss function. Usually, we denote the gradient with the symbol $\\nabla$.\n",
    "\n",
    "\n",
    "In a single dimension, the gradient of a function $f(x)$ with respect to $x$ is the derivative - $\\frac {df} {dx}$. It represents the rate of change in the function at a specific point and indicates the direction of the steepest ascent."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc4df5f9646f7432"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In multiple dimensions, the gradient generalizes to a vector of partial derivatives. For a function $f(x)$ where $x=[x_1,x_2,...x_n]$ is an $n$-dimensional vector, the gradient is a vector of the form:\n",
    "$\\nabla f(x)=\\left[\\frac{\\partial f}{\\partial x_{1}},\\frac{\\partial f}{\\partial x_{2}},...,\\frac{\\partial f}{\\partial x_{n}}\\right]$\n",
    "\n",
    "Each component $\\frac{\\partial f}{\\partial x_{i}} $ represents the rate of change of $f$ with respect to the variable $x_{i}$. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "624e340c4fe0afcd"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f([3. 4. 5.]) = 660.0, Gradient of f at [3. 4. 5.] is [  3.  16. 375.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Say we have the function f(x,y,z) = x + 2y**2 + 5z**3. Calculate the gradient. \"\"\"\n",
    "# Define the function f\n",
    "def f(v):\n",
    "    x, y, z = v\n",
    "    return x + 2 * (y ** 2) + 5 * (z ** 3)\n",
    "\n",
    "\n",
    "# Define the function to compute the gradient of f\n",
    "def grad_f(v):\n",
    "    x, y, z = v\n",
    "    df_dx = x\n",
    "    df_dy = 2 * (2 * y)\n",
    "    df_dz = 5 * (3 * z**2)\n",
    "    return np.array([df_dx, df_dy, df_dz])\n",
    "\n",
    "# Usage\n",
    "v = np.array([3.0, 4.0, 5.0])\n",
    "function_value = f(v)\n",
    "gradient = grad_f(v)\n",
    "print(f\"f({v}) = {function_value}, Gradient of f at {v} is {gradient}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T17:02:40.723734900Z",
     "start_time": "2024-07-25T17:02:40.666015Z"
    }
   },
   "id": "e49b15edc4f6d1b8"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# This was a gradient of a very specific case.\n",
    "# This time, we will use numpy function. #todo\n",
    "\n",
    "gradient = np.gradient(v)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-25T17:02:40.723734900Z",
     "start_time": "2024-07-25T17:02:40.684595200Z"
    }
   },
   "id": "89b74465093ef4a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### \n",
    "Properties of Gradient:\n",
    "1. Direction of Steepest Ascent: The gradient vector points in the direction of the steepest increase  the function. Moving in the opposite direction of the gradient leads to the steepest decrease, which is used in optimization algorithms like gradient descent.\n",
    "\n",
    "\n",
    "2. Magnitude and Direction: The magnitude of the gradient vector indicates how steep the slope is. A larger magnitude means a steeper slope, while a smaller magnitude indicates a flatter region."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbc86f0f053972c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning Rate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d38d093735ef6b69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The learning rate is a hyperparameter in the training of neural networks and other machine learning models. \n",
    "It determines the size of the steps the model takes to update the weights in response to the error computed during training. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf8e422f71fdcd94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- If the learning rate is too high, the model may take steps that are too large and overshoot the optimal point. This can cause the loss function to oscillate or even diverge, failing to converge to a minimum.\n",
    "- If the learning rate is too low, the model will take tiny steps, making the training process slow. It might get stuck in local minima and may take a long time to converge to the global minimum, if at all.\n",
    "- An optimal learning rate is one that is small enough to ensure convergence and large enough to make the training process efficient. Finding this optimal value often requires experimentation and tuning."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6be421c616f5710a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Using Learning Rate And gradient To Update The weight, in \"Gradient Decent\"\n",
    "Every time \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ed2132c4e5ab32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Components of a Neural Network \n",
    "1. Neurons: Basic units (node) of a neural network that receive input, process it, and pass it on to other neurons. Each neuron has weights and biases that are adjusted during training to minimize errors. The neurons organized in layers. \n",
    "\n",
    "\n",
    "\n",
    "2. Layers: There is three types of Layers:\n",
    "    - Input Layer: The first layer that receives the input data.\n",
    "    - Hidden Layers: Intermediate layers that process inputs from the input layer. There can be one or more hidden layers in a neural network.\n",
    "    - Output Layer: The final layer that produces the output predictions.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "957e7bb23b849080"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
