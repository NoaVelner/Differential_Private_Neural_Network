{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:11:36.200858Z",
     "start_time": "2024-08-06T07:11:35.315912900Z"
    }
   },
   "id": "b59d16b20e3ffcef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction To neural Networks  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ed0d840c2d252c6c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### What is a Neural Network?\n",
    "A neural network is a series of algorithms that attempt to recognize underlying relationships in a set of data through a process. The network consists of layers of nodes, or neurons, each performing simple computations and passing the results to the next layer."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc80eea718403aa5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Basic Terms"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "206620def2c9f0a6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loss\n",
    " The difference between the actual output and the predicted output is calculated using a loss function. This error measure guides the learning process.\n",
    " \n",
    " \n",
    "We can think at loss that simply sums the ${\\ell}_2$ distance between the predicted output form the actual output:\n",
    "$MSE={\\frac 1 n} \\sum ^n _{i=1} (y_i - \\hat y_i)^2$\n",
    "\n",
    "In neural network, we will use the \"Cross-Entropy Loss\".\n",
    "Cross-Entropy Loss: Used for classification tasks, it measures the difference between two probability distributions – the true labels and the predicted probabilities\n",
    "${\\text {Cross-Entropy Loss} = -\\sum ^n _{i=1}[y_i \\cdot log(\\hat {y_i})+(1-y_i)\\cdot log(1-\\hat{y_i})]$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f11109cd5a39fd2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Role in Training: The loss function guides the optimization process. During training, the goal is to minimize the loss, which means making the predictions as accurate as possible.\n",
    "\n",
    "- Loss Curve: A plot of loss versus training epochs can help visualize how well the model is learning. A decreasing loss indicates that the model is improving."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1eb94c5c76a253d3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Task: implement cross entropy loss.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6c29206c89ba10f7"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def crossEntropy_loss(output, targets, buffer):\n",
    "    \"\"\"\n",
    "    Calculates the categorical cross-entropy loss.\n",
    "\n",
    "    Args:\n",
    "        Output (Tensor): Model predictions of shape (num_samples, output_size).\n",
    "        Targets (Tensor): Target labels of shape (num_samples, output_size).\n",
    "\n",
    "    Returns:\n",
    "        float: Categorical cross-entropy loss value.\n",
    "    \"\"\"\n",
    "    # buffer = 1e-10\n",
    "    loss = -np.mean(targets * np.log(output + buffer))\n",
    "    return loss"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:51:19.326775500Z",
     "start_time": "2024-08-06T06:51:19.323187600Z"
    }
   },
   "id": "aa9a03c8605c4e6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Accuracy \n",
    "Another way to measure our model, is $accuracy$.\n",
    "Accuracy is a metric used to evaluate the performance of a classification model. It measures the proportion of correct predictions out of the total number of predictions.\n",
    "Accuracy is calculated as the number of correct predictions divided by the total number of predictions. It is often expressed as a percentage.\n",
    " \n",
    "\n",
    "$Accuracy = {\\frac {\\text {Number of Correct Predictions}} {\\text {Total Number of Predictions}} \\cdot 100}$ \n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f42d1a90e808bf55"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If $y_i$ is the actual label and $\\hat {y_i}$ is the predicted label for $n$ samples:\n",
    "$Accuracy = {\\frac 1 n} \\sum ^n _{i=1} \\mathds{1}(y_i = \\hat{y_i})$ , where 1 is the indicator function, which is 1 if the condition is true and 0 otherwise."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c5ce71e550e21061"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Implement the following accuracy function for classification problem (possible labels are 0,1) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d4e592798a102027"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def accuracy(output: np.ndarray, targets: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the accuracy of the model predictions.\n",
    "\n",
    "    Args:\n",
    "        Output (Tensor): Model predictions of shape (num_samples, output_size).\n",
    "        Targets (Tensor): Target labels of shape (num_samples, output_size).\n",
    "\n",
    "    Returns:\n",
    "        float: Accuracy value.\n",
    "    \"\"\"\n",
    "    predicted_labels = np.argmax(output, axis=1)\n",
    "    true_labels = np.argmax(targets, axis=1)\n",
    "    return np.mean(predicted_labels == true_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:51:19.341850700Z",
     "start_time": "2024-08-06T06:51:19.326775500Z"
    }
   },
   "id": "ecbde8ba1c82533d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Activation functions\n",
    "Activation functions play a crucial role in neural networks by introducing non-linearity into the model. This non-linearity allows the network to learn and model complex relationships between inputs and outputs. Here’s a detailed overview of various activation functions and their roles:\n",
    "\n",
    "1. Non-linearity: Without activation functions, a neural network would perform linear transformations, making it incapable of solving non-linear problems.\n",
    "2. Enabling Learning: Activation functions enable backpropagation by providing gradients needed for updating weights.\n",
    "3. Controlling Outputs: They help in squashing the output to a specific range, making the network's behavior more predictable and stable."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71a8997e542557da"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Common Activation Functions:\n",
    "- sigmoid/logistic function: $\\sigma (x) =  \\frac 1 {1+e^{-x}}$.\n",
    "  Properties:\n",
    "  - Outputs values between 0 and 1.\n",
    "  - Smooth gradient, preventing abrupt changes in output.\n",
    "- Rectified Linear Unit (ReLU):$\n",
    "\n",
    "\\text{ReLU}(x)\t=max\\left\\{ 0,x\\right\\} $.\n",
    "This function gives us:\n",
    "    - outputs values between zero and infinity.\n",
    "    - Introduces sparsity by setting negative values to zero.\n",
    "- SoftMax\n",
    "    $\\text{Softmax}(x_i)=\\frac {e^{x_i}} {\\sum ^C _{j=1}e^{x_j}} \\text{(where C is the number of classes).}$\n",
    "    Properties:\n",
    "    - Outputs a probability distribution over classes.\n",
    "    - Commonly used in the output layer of multi-class classification problems\n",
    "    - Provides probabilistic interpretation.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a6203d465a8220c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_values / np.sum(exp_values, axis=-1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:51:19.348407Z",
     "start_time": "2024-08-06T06:51:19.341850700Z"
    }
   },
   "id": "8582d1083883d59e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Gradient\n",
    "Gradient is a vector that represents the direction and rate of the fastest increase a function. In neural networks, it is used to adjust the weights and biases to minimize the loss function. Usually, we denote the gradient with the symbol $\\nabla$.\n",
    "\n",
    "\n",
    "In a single dimension, the gradient of a function $f(x)$ with respect to $x$ is the derivative - $\\frac {df} {dx}$. It represents the rate of change in the function at a specific point and indicates the direction of the steepest ascent."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dc4df5f9646f7432"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In multiple dimensions, the gradient generalizes to a vector of partial derivatives. For a function $f(x)$ where $x=[x_1,x_2,...x_n]$ is an $n$-dimensional vector, the gradient is a vector of the form:\n",
    "$\\nabla f(x)=\\left[\\frac{\\partial f}{\\partial x_{1}},\\frac{\\partial f}{\\partial x_{2}},...,\\frac{\\partial f}{\\partial x_{n}}\\right]$\n",
    "\n",
    "Each component $\\frac{\\partial f}{\\partial x_{i}} $ represents the rate of change of $f$ with respect to the variable $x_{i}$. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "624e340c4fe0afcd"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f([3. 4. 5.]) = 660.0, Gradient of f at [3. 4. 5.] is [  3.  16. 375.]\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Say we have the function f(x,y,z) = x + 2y**2 + 5z**3. Calculate the gradient. \"\"\"\n",
    "# Define the function f\n",
    "def f(v):\n",
    "    x, y, z = v\n",
    "    return x + 2 * (y ** 2) + 5 * (z ** 3)\n",
    "\n",
    "\n",
    "# Define the function to compute the gradient of f\n",
    "def grad_f(v):\n",
    "    x, y, z = v\n",
    "    df_dx = x\n",
    "    df_dy = 2 * (2 * y)\n",
    "    df_dz = 5 * (3 * z**2)\n",
    "    return np.array([df_dx, df_dy, df_dz])\n",
    "\n",
    "# Usage\n",
    "v = np.array([3.0, 4.0, 5.0])\n",
    "function_value = f(v)\n",
    "gradient = grad_f(v)\n",
    "print(f\"f({v}) = {function_value}, Gradient of f at {v} is {gradient}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:51:19.433218300Z",
     "start_time": "2024-08-06T06:51:19.348407Z"
    }
   },
   "id": "e49b15edc4f6d1b8"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# This was a gradient of a very specific case.\n",
    "# This time, we will use numpy function. #todo\n",
    "\n",
    "gradient = np.gradient(v)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T06:51:19.448842900Z",
     "start_time": "2024-08-06T06:51:19.362749600Z"
    }
   },
   "id": "89b74465093ef4a5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### \n",
    "Properties of Gradient:\n",
    "1. Direction of Steepest Ascent: The gradient vector points in the direction of the steepest increase  the function. Moving in the opposite direction of the gradient leads to the steepest decrease, which is used in optimization algorithms like gradient descent.\n",
    "\n",
    "\n",
    "2. Magnitude and Direction: The magnitude of the gradient vector indicates how steep the slope is. A larger magnitude means a steeper slope, while a smaller magnitude indicates a flatter region."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fbc86f0f053972c8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Learning Rate"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d38d093735ef6b69"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The learning rate is a hyperparameter in the training of neural networks and other machine learning models. \n",
    "It determines the size of the steps the model takes to update the weights in response to the error computed during training. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bf8e422f71fdcd94"
  },
  {
   "cell_type": "markdown",
   "source": [
    "- If the learning rate is too high, the model may take steps that are too large and overshoot the optimal point. This can cause the loss function to oscillate or even diverge, failing to converge to a minimum.\n",
    "- If the learning rate is too low, the model will take tiny steps, making the training process slow. It might get stuck in local minima and may take a long time to converge to the global minimum, if at all.\n",
    "- An optimal learning rate is one that is small enough to ensure convergence and large enough to make the training process efficient. Finding this optimal value often requires experimentation and tuning."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6be421c616f5710a"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f00b5d3475719ae9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Using Learning Rate And Gradient To Update Weights In \"Gradient Decent\"\n",
    "1. Start at a Random Point: Begin with some random weights.\n",
    "2. Calculate the Gradient: Measure the slope (gradient) of the hill (error function) at your current point.\n",
    "3. Update Weights: Adjust your weights in the direction that reduces errors. The learning rate controls the size of your step.\n",
    "4. Repeat: Keep doing this until you can't reduce the errors any further.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1ed2132c4e5ab32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Components of a Neural Network \n",
    "1. Neurons: Basic units (node) of a neural network that receive input, process it, and pass it on to other neurons. Each neuron has weights and biases that are adjusted during training to minimize errors. The neurons organized in layers. \n",
    "\n",
    "\n",
    "\n",
    "2. Layers: There is three types of Layers:\n",
    "    - Input Layer: The first layer that receives the input data.\n",
    "    - Hidden Layers: Intermediate layers that process inputs from the input layer. There can be one or more hidden layers in a neural network.\n",
    "    - Output Layer: The final layer that produces the output predictions.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "957e7bb23b849080"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In our implementation, we can create a layer as following:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "558d6a0d5c0e8be1"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Layer.FullyConnectedLayer at 0x1d5d4450910>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Layer \n",
    "import Train\n",
    "\n",
    "input_size = 784\n",
    "output_size = 512\n",
    "Layer.FullyConnectedLayer(input_size=input_size, output_size=output_size, activation=\"relu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:11:42.779147100Z",
     "start_time": "2024-08-06T07:11:41.660646900Z"
    }
   },
   "id": "97ff68a6300b8945"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the following structure:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0064f06db3827b3"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "hidden_size = [512, 512]\n",
    "output_size = 10\n",
    "\n",
    "layer1 = Layer.FullyConnectedLayer(input_size=input_size, output_size=hidden_size[0], activation=\"relu\")\n",
    "layer2 = Layer.FullyConnectedLayer(input_size=hidden_size[0], output_size=hidden_size[1], activation=\"relu\")\n",
    "layer3 = Layer.FullyConnectedLayer(input_size=hidden_size[1], output_size=output_size, activation=\"softmax\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:11:45.384299500Z",
     "start_time": "2024-08-06T07:11:45.344807400Z"
    }
   },
   "id": "66440d3a6ba7498a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## MNIST\n",
    "The MNIST dataset is one of the most well-known and widely used datasets in the field of machine learning and computer vision. It is a large database of handwritten digits that is commonly used for training and testing image processing systems.\n",
    "\n",
    "### Key Features of the MNIST Dataset:\n",
    "\n",
    "**Dataset Composition**:\n",
    "   - The dataset consists of 70,000 grayscale images of handwritten digits from 0 to 9.\n",
    "   - Each image is 28x28 pixels in size, making it relatively small and easy to process.\n",
    "   - The images are already preprocessed and normalized. Each pixel value ranges from 0 to 255, where 0 represents white and 255 represents black.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3579b715e1da5c0"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 10 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = Train.load_mnist()\n",
    "\n",
    "input_shape = 784\n",
    "hidden_shape = [512, 512]\n",
    "output_shape = 10\n",
    "x_test = x_test.reshape((x_test.shape[0], -1))\n",
    "y_test = to_categorical(y_test, num_classes=output_shape)\n",
    "\n",
    "\n",
    "for idx, i in enumerate([i for i in range(10)]):\n",
    "    plt.subplot(1, 10, idx + 1)  \n",
    "    first_image = x_test[i]\n",
    "\n",
    "    first_image = first_image.reshape((28, 28))\n",
    "    plt.imshow(first_image, cmap='Greys', interpolation='None')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show() "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:17:44.467500700Z",
     "start_time": "2024-08-06T07:17:43.469100600Z"
    }
   },
   "id": "f57f7d02faacb71f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "To simplify that, we have Train.load_mnist() function. It will split data to Train set and Test set, normalize it and make the label categorical. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "278ad81cd5516d32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can start creating MNIST models.\n",
    "In this model, we have three layers: input, hidden layer and output layer. Both input and hidden layers with 'relu' activation function, and the output layer with 'softmax' activation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bd11cecf6e7a077"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "nn = Train.CreateModel(input_size=input_shape, output_size=output_shape, hidden_size=hidden_shape, noise=0.01)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T07:22:23.908024400Z",
     "start_time": "2024-08-06T07:22:23.860616200Z"
    }
   },
   "id": "a48c15cbd9426334"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to update the weight between the lines, we need a method to teach the model. It is called **forward and backward propagation**.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a2b2082e0e1235"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Forward Propagation\n",
    "\n",
    "Forward propagation is the process by which a neural network makes predictions. It involves passing input data through the network's layers, computing the output at each layer, and ultimately producing a final prediction. \n",
    "\n",
    "\n",
    "1. The process begins with the input data being fed into the input layer. Each input feature corresponds to a neuron in the input layer.\n",
    "2. In each neuron of the hidden and output layers, a weighted sum of the inputs is calculated. This is done using the formula: \n",
    "    $z=\\sum_{i}( x_{i},w_{i}) +b\\\\$\n",
    "   where $w_i$ are the weights, $x_i$ are the input values, and $b$ is the bias term.\n",
    "\n",
    "3. The weighted sum $z$ is then passed through an activation function to introduce non-linearity into the model. \n",
    "$a = \\varphi(z)$, where $\\varphi$ is the activation function. \n",
    "The activated output $a$ from the current layer serves as the input to the next layer. This process is repeated for all hidden layers."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "561ce9680e7d2cf7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### implement forward propagation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ef026ca5042c201"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "def forward(self, x: np.ndarray) -> np.ndarray:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-06T08:28:05.700701800Z",
     "start_time": "2024-08-06T08:28:05.683999400Z"
    }
   },
   "id": "925d5ee848315f2e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Backpropagation\n",
    "\n",
    "Backpropagation is an algorithm used for training neural networks. It involves adjusting the weights and biases of the network in order to minimize the difference between the predicted output and the actual output (the error). \n",
    "This process is the counterpart to forward propagation and works by propagating the error backward through the network.\n",
    "\n",
    "\n",
    "#### **Steps of Backpropagation:**\n",
    "1. **Forward Propagation**:\n",
    "    - Performs forward propagation to compute the predicted output of the network for the given input data.\n",
    "    - Calculate the loss using the loss function.\n",
    "2. **Backward Propagation**:\n",
    "    Compute the gradient of the loss function with respect to the output of each neuron, layer by layer, starting from the output layer and moving backward to the input layer.\n",
    "3. **Gradient Calculation**:\n",
    "    - Hidden Layers: \n",
    "      For each hidden layer, compute the gradient of the loss with respect to the inputs to that layer, using the chain rule to propagate the error backward.\n",
    "          $\\delta^{l}=\\left(\\frac{\\partial Loss}{\\partial a^{l}}\\right)\\cdot\\left(\\frac{\\partial a^{l}}{\\partial z^{l}}\\right)$\n",
    "              where $\\delta^l$ is the error term for layer $l$, $a^l$ is the activation, and $z^l$ is the pre-activation value.\n",
    "    - Output Layer: \n",
    "      Compute the gradient of the loss with respect to the output layer's inputs (pre-activation values). This involves differentiating the loss function. $\\delta=\\frac{\\partial Loss}{\\partial z}$\n",
    "4. **Update the weights and biases using the computed gradients.**\n",
    "    This is typically done using an optimization algorithm like Gradient Descent.\n",
    "    $ \\[\n",
    "     w_{ij}^{l} = w_{ij}^{l} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial w_{ij}^{l}}\n",
    "     \\]\n",
    "     \\[\n",
    "     b_j^{l} = b_j^{l} - \\eta \\cdot \\frac{\\partial \\text{Loss}}{\\partial b_j^{l}}\n",
    "     \\]$\n",
    "     where $\\( \\eta \\)$ is the learning rate, $\\( w_{ij}^{l} \\)$ is the weight connecting neuron $\\( i \\)$ in layer $\\( l-1 \\)$ to neuron $\\( j \\)$ in layer $\\( l \\)$, and $\\( b_j^{l} \\)$ is the bias of neuron $\\( j \\)$ in layer $\\( l \\)$.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "65ec4d89ab16212d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here's a code snip from 'Train'."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5188751c71dae4ee"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "    def backward(self, d_values: np.ndarray, learning_rate: float, t: int):\n",
    "        \"\"\"\n",
    "        Backpropagation.\n",
    "        This function will derivative the activation function, and then calculate the\n",
    "        derivative once with respect to the bias and oe with respect to the weight.\n",
    "        Those values might be very small, so we will clip them to keep numerical stability.\n",
    "        \"\"\"\n",
    "        # step 1: Compute the Gradient of the Activation Function\n",
    "        d_values = self.derivative_activation_function(d_values)\n",
    "        \n",
    "        # step 2: Compute Gradients for Weights and Biases\n",
    "        d_weights = self.x.T @ d_values\n",
    "        d_biases = np.sum(d_values, axis=0, keepdims=True)\n",
    "\n",
    "\n",
    "        # step 3: Calculate the gradient with respect to the input\n",
    "        d_inputs = d_values @ self.weights.T\n",
    "\n",
    "        # Step 4: Update the weights and biases using the learning rate\n",
    "        self.weights -= learning_rate * d_weights\n",
    "        self.biases -= learning_rate * d_biases\n",
    "\n",
    "        # Update weights & biases using m and v values\n",
    "        self._update_parameters({'param': self.weights, 'm': self.m_weights, 'v': self.v_weights}, d_weights, t,\n",
    "                                learning_rate)\n",
    "        self._update_parameters({'param': self.biases, 'm': self.m_biases, 'v': self.v_biases}, d_biases, t,\n",
    "                                learning_rate)\n",
    "        return d_inputs"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "44e54ae34e6aaf30"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2570de164dd7ec18"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "89ef1ac8e27198ad"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
